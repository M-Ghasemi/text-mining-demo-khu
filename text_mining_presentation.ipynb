{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Mining\n",
    "\n",
    "### Mohammad Sadegh Ghasemi\n",
    "#### this project is based on the \"Text Analytics with Python\" book\n",
    "#### Created for \"Computational Data Mining\", Kharazmi University, 96-1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importin required libraries/codes\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "import scipy.sparse as sp\n",
    "\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# query section required functions\n",
    "from normalization import parse_document, normalize_corpus, normalize_document\n",
    "from feature_extractors import build_feature_matrix\n",
    "\n",
    "# Topic Modeling\n",
    "from gensim import corpora, models\n",
    "from utils import low_rank_svd\n",
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extraction codes\n",
    "\n",
    "def bow_extractor(corpus, ngram_range=(1, 1)):\n",
    "\n",
    "    vectorizer = CountVectorizer(min_df=1, ngram_range=ngram_range)\n",
    "    features = vectorizer.fit_transform(corpus)\n",
    "    return vectorizer, features\n",
    "\n",
    "\n",
    "def tfidf_transformer(bow_matrix):\n",
    "\n",
    "    transformer = TfidfTransformer(norm='l2',\n",
    "                                   smooth_idf=True,\n",
    "                                   use_idf=True)\n",
    "    tfidf_matrix = transformer.fit_transform(bow_matrix)\n",
    "    return transformer, tfidf_matrix\n",
    "\n",
    "\n",
    "def tfidf_extractor(corpus, ngram_range=(1, 1)):\n",
    "\n",
    "    vectorizer = TfidfVectorizer(min_df=1,\n",
    "                                 norm='l2',\n",
    "                                 smooth_idf=True,\n",
    "                                 use_idf=True,\n",
    "                                 ngram_range=ngram_range)\n",
    "    features = vectorizer.fit_transform(corpus)\n",
    "    return vectorizer, features\n",
    "\n",
    "\n",
    "def average_word_vectors(words, model, vocabulary, num_features):\n",
    "\n",
    "    feature_vector = np.zeros((num_features,), dtype=\"float64\")\n",
    "    nwords = 0.\n",
    "\n",
    "    for word in words:\n",
    "        if word in vocabulary:\n",
    "            nwords = nwords + 1.\n",
    "            feature_vector = np.add(feature_vector, model[word])\n",
    "\n",
    "    if nwords:\n",
    "        feature_vector = np.divide(feature_vector, nwords)\n",
    "\n",
    "    return feature_vector\n",
    "\n",
    "\n",
    "def averaged_word_vectorizer(corpus, model, num_features):\n",
    "    vocabulary = set(model.index2word)\n",
    "    features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features)\n",
    "                for tokenized_sentence in corpus]\n",
    "    return np.array(features)\n",
    "\n",
    "\n",
    "def tfidf_wtd_avg_word_vectors(words, tfidf_vector, tfidf_vocabulary, model, num_features):\n",
    "\n",
    "    word_tfidfs = [tfidf_vector[0, tfidf_vocabulary.get(word)]\n",
    "                   if tfidf_vocabulary.get(word)\n",
    "                   else 0 for word in words]\n",
    "    word_tfidf_map = {word: tfidf_val for word, tfidf_val in zip(words, word_tfidfs)}\n",
    "\n",
    "    feature_vector = np.zeros((num_features,), dtype=\"float64\")\n",
    "    vocabulary = set(model.index2word)\n",
    "    wts = 0.\n",
    "    for word in words:\n",
    "        if word in vocabulary:\n",
    "            word_vector = model[word]\n",
    "            weighted_word_vector = word_tfidf_map[word] * word_vector\n",
    "            wts = wts + word_tfidf_map[word]\n",
    "            feature_vector = np.add(feature_vector, weighted_word_vector)\n",
    "    if wts:\n",
    "        feature_vector = np.divide(feature_vector, wts)\n",
    "\n",
    "    return feature_vector\n",
    "\n",
    "\n",
    "def tfidf_weighted_averaged_word_vectorizer(corpus, tfidf_vectors,\n",
    "                                            tfidf_vocabulary, model, num_features):\n",
    "\n",
    "    docs_tfidfs = [(doc, doc_tfidf)\n",
    "                   for doc, doc_tfidf\n",
    "                   in zip(corpus, tfidf_vectors)]\n",
    "    features = [tfidf_wtd_avg_word_vectors(tokenized_sentence, tfidf, tfidf_vocabulary,\n",
    "                                           model, num_features)\n",
    "                for tokenized_sentence, tfidf in docs_tfidfs]\n",
    "    return np.array(features)\n",
    "\n",
    "\n",
    "def display_features(features, feature_names):\n",
    "    df = pd.DataFrame(data=features,\n",
    "                      columns=feature_names)\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Normalization functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    sentence_tokens = nltk.sent_tokenize(text)\n",
    "    word_tokens = [nltk.word_tokenize(sentence) for sentence in sentence_tokens]\n",
    "    return sentence_tokens, word_tokens\n",
    "\n",
    "\n",
    "def remove_characters_after_tokenization(tokens):\n",
    "    pattern = re.compile('[{}]'.format(re.escape(string.punctuation)))\n",
    "    filtered_tokens = list(filter(None, [pattern.sub('', token) for token in tokens]))\n",
    "    return filtered_tokens\n",
    "\n",
    "\n",
    "def remove_characters_before_tokenization(sentence, keep_apostrophes=False):\n",
    "    sentence = sentence.strip()\n",
    "    if keep_apostrophes:\n",
    "        PATTERN = r'[?|$|&|*|%|@|(|)|~]'  # add other characters here to remove them\n",
    "        filtered_sentence = re.sub(PATTERN, r'', sentence)\n",
    "    else:\n",
    "        PATTERN = r'[^a-zA-Z0-9 ]'  # only extract alpha-numeric characters\n",
    "        filtered_sentence = re.sub(PATTERN, r'', sentence)\n",
    "    return filtered_sentence\n",
    "\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    stopword_list = nltk.corpus.stopwords.words('english')\n",
    "    filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    return filtered_tokens\n",
    "\n",
    "\n",
    "def remove_repeated_characters(tokens):\n",
    "    repeat_pattern = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "    match_substitution = r'\\1\\2\\3'\n",
    "\n",
    "    def replace(old_word):\n",
    "        if wordnet.synsets(old_word):\n",
    "            return old_word\n",
    "        new_word = repeat_pattern.sub(match_substitution, old_word)\n",
    "        return replace(new_word) if new_word != old_word else new_word\n",
    "\n",
    "    correct_tokens = [replace(word) for word in tokens]\n",
    "    return correct_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading a corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corpus(name):\n",
    "\n",
    "    corpus = []\n",
    "    with open(name, 'r') as corpus_file:\n",
    "        for doc in corpus_file:\n",
    "            if doc.strip():\n",
    "                corpus.append(parse_document(doc))\n",
    "        corpus_file.close()\n",
    "        return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## searching between corpus documents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(corpus):\n",
    "    \"\"\"Search Demo function\"\"\"\n",
    "\n",
    "    normalized_corpus = normalize_corpus(corpus)\n",
    "    vectorizer, feature_matrix = build_feature_matrix(\n",
    "        normalized_corpus, 'tfidf')\n",
    "\n",
    "    q = input('Enter search query. Press \"Enter\" to stop: \\n')\n",
    "\n",
    "    while q != '':\n",
    "        q = normalize_document(q)\n",
    "        q_tfidf = vectorizer.transform([q])\n",
    "        ans_mat = q_tfidf.dot(feature_matrix.transpose())\n",
    "        ans_list = []\n",
    "        for j in range(ans_mat.shape[1]):\n",
    "            if ans_mat[0, j] > 0:\n",
    "                ans_list.append((j, ans_mat[0, j]))\n",
    "        ans_list.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        print()\n",
    "        print('************ {} ************'.format(q))\n",
    "\n",
    "        for item in ans_list[:5]:\n",
    "            print()\n",
    "            print('Document no. {}, rank: {}'.format(item[0], item[1]))\n",
    "            print(corpus[item[0]][:])\n",
    "            print()\n",
    "            print()\n",
    "\n",
    "        q = input('Enter search query. Press \"Enter\" to stop: \\n')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic modeling functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_topics_gensim(topic_model, total_topics=1,\n",
    "                        weight_threshold=0.0001,\n",
    "                        display_weights=False,\n",
    "                        num_terms=None):\n",
    "\n",
    "    for index in range(total_topics):\n",
    "        topic = topic_model.show_topic(index)\n",
    "        topic = [(word, round(wt, 2))\n",
    "                 for word, wt in topic\n",
    "                 if abs(wt) >= weight_threshold]\n",
    "        if display_weights:\n",
    "            print('Topic #' + str(index + 1) + ' with weights')\n",
    "            print(topic[:num_terms] if num_terms else topic)\n",
    "        else:\n",
    "            print('Topic #' + str(index + 1) + ' without weights')\n",
    "            tw = [term for term, wt in topic]\n",
    "            print(tw[:num_terms] if num_terms else tw)\n",
    "        print()\n",
    "\n",
    "\n",
    "def get_topics_terms_weights(weights, feature_names):\n",
    "    feature_names = np.array(feature_names)\n",
    "    sorted_indices = np.array([list(row[::-1])\n",
    "                               for row\n",
    "                               in np.argsort(np.abs(weights))])\n",
    "    sorted_weights = np.array([list(wt[index])\n",
    "                               for wt, index\n",
    "                               in zip(weights, sorted_indices)])\n",
    "    sorted_terms = np.array([list(feature_names[row])\n",
    "                             for row\n",
    "                             in sorted_indices])\n",
    "\n",
    "    topics = [np.vstack((terms.T,\n",
    "                         term_weights.T)).T\n",
    "              for terms, term_weights\n",
    "              in zip(sorted_terms, sorted_weights)]\n",
    "\n",
    "    return topics\n",
    "\n",
    "\n",
    "def print_topics_udf(topics, total_topics=1,\n",
    "                     weight_threshold=0.0001,\n",
    "                     display_weights=False,\n",
    "                     num_terms=None):\n",
    "\n",
    "    for index in range(total_topics):\n",
    "        topic = topics[index]\n",
    "        topic = [(term, float(wt))\n",
    "                 for term, wt in topic]\n",
    "        topic = [(word, round(wt, 2))\n",
    "                 for word, wt in topic\n",
    "                 if abs(wt) >= weight_threshold]\n",
    "\n",
    "        if display_weights:\n",
    "            print('Topic #' + str(index + 1) + ' with weights')\n",
    "            print(topic[:num_terms] if num_terms else topic)\n",
    "        else:\n",
    "            print('Topic #' + str(index + 1) + ' without weights')\n",
    "            tw = [term for term, wt in topic]\n",
    "            print(tw[:num_terms] if num_terms else tw)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'brown', 'fox', 'was', \"n't\", 'that', 'quick', 'and', 'he', 'could', \"n't\", 'win', 'the', 'race']\n"
     ]
    }
   ],
   "source": [
    "# Word TOKENIZATION\n",
    "sentence = \"The brown fox wasn't that quick and he couldn't win the race\"\n",
    "words = nltk.word_tokenize(sentence)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will discuss briefly about the basic syntax, structure and design philosophies.  There is a defined hierarchical syntax for Python code which you should remember  when writing code! Python is a really powerful programming language!\n",
      "\n",
      "Total sentences in sample_text: 3\n",
      "\n",
      "('We will discuss briefly about the basic syntax, structure and design '\n",
      " 'philosophies.')\n"
     ]
    }
   ],
   "source": [
    "# Sentence TOKENIZATION\n",
    "sample_text = 'We will discuss briefly about the basic syntax,\\\n",
    " structure and design philosophies. \\\n",
    " There is a defined hierarchical syntax for Python code which you should remember \\\n",
    " when writing code! Python is a really powerful programming language!'\n",
    "sentences = nltk.sent_tokenize(sample_text)\n",
    "print(sample_text)\n",
    "print('\\nTotal sentences in sample_text:', len(sentences))\n",
    "print()\n",
    "pprint(sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We _ ##will discuss @*briefly about the basic syntax, structure and design philosophies.\n",
      "\n",
      "['We', '_', '#', '#', 'will', 'discuss', '@', '*briefly', 'about', 'the', 'basic', 'syntax', ',', 'structure', 'and', 'design', 'philosophies', '.']\n"
     ]
    }
   ],
   "source": [
    "# Sentence and Word TOKENIZATION\n",
    "another_text = 'We _ ##will discuss @*briefly about the basic syntax,\\\n",
    " structure and design philosophies. \\\n",
    " There is a defined $hierarchical syntax for Python code which you should remember \\\n",
    " when writing code! @@Python (is) a **really** powerful programming language!@@'\n",
    "sentences, words = tokenize_text(another_text)\n",
    "print(sentences[0])\n",
    "print()\n",
    "print(words[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@@Python (is) a **really** powerful programming language!\n",
      "\n",
      "Python is a really powerful programming language\n"
     ]
    }
   ],
   "source": [
    "# Cleaning\n",
    "cleaned_sentences = [remove_characters_before_tokenization(s) for s in sentences]\n",
    "print(sentences[2])\n",
    "print()\n",
    "print(cleaned_sentences[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['available', 'google']\n"
     ]
    }
   ],
   "source": [
    "# Removing Stopwords\n",
    "\n",
    "words = ['am', 'a', 'then', 'so', 'available', 'google']\n",
    "print(remove_stopwords(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing repeated characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My name is realy amazing']\n"
     ]
    }
   ],
   "source": [
    "# removing repeated characters\n",
    "\n",
    "sample_sentence = 'Myyyy nameee is realllllyyy amaaazingggg'\n",
    "sample_sentence_tokens = tokenize_text(sample_sentence)[0]\n",
    "\n",
    "print(remove_repeated_characters(sample_sentence_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activ activ activ activ\n",
      "jump jump jump\n",
      "lie\n",
      "strang\n"
     ]
    }
   ],
   "source": [
    "# porter stemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "print(\n",
    "    ps.stem('activation'),\n",
    "    ps.stem('active'),\n",
    "    ps.stem('actively'),\n",
    "    ps.stem('activities'))\n",
    "\n",
    "print(\n",
    "    ps.stem('jumping'),\n",
    "    ps.stem('jumps'),\n",
    "    ps.stem('jumped'))\n",
    "\n",
    "print(ps.stem('lying'))\n",
    "\n",
    "print(ps.stem('strange'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatization\n",
    "\n",
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "car\n",
      "men\n"
     ]
    }
   ],
   "source": [
    "# lemmatize nouns\n",
    "print(wnl.lemmatize('cars', 'n'))\n",
    "print(wnl.lemmatize('men', 'n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "eat\n"
     ]
    }
   ],
   "source": [
    "# lemmatize verbs\n",
    "print(wnl.lemmatize('running', 'v'))\n",
    "print(wnl.lemmatize('ate', 'v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sad\n",
      "fancy\n"
     ]
    }
   ],
   "source": [
    "# lemmatize adjectives\n",
    "print(wnl.lemmatize('saddest', 'a'))\n",
    "print(wnl.lemmatize('fancier', 'a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ate\n",
      "fancier\n"
     ]
    }
   ],
   "source": [
    "# ineffective lemmatization\n",
    "print(wnl.lemmatize('ate', 'n'))\n",
    "print(wnl.lemmatize('fancier', 'v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Space Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   and  beautiful  blue  cheese  is  love  sky  so  the\n",
      "0    0          0     1       0   1     0    1   0    1\n",
      "1    1          1     1       0   2     0    2   0    0\n",
      "2    0          1     1       0   1     0    1   1    1\n",
      "3    0          0     1       1   0     1    0   0    0\n"
     ]
    }
   ],
   "source": [
    "# Vector Space Model\n",
    "CORPUS = [\n",
    "    'the sky is blue',\n",
    "    'sky is blue and sky is beautiful',\n",
    "    'the beautiful sky is so blue',\n",
    "    'i love blue cheese'\n",
    "]\n",
    "\n",
    "bow_vectorizer, bow_features = bow_extractor(CORPUS)\n",
    "\n",
    "feature_names = bow_vectorizer.get_feature_names()\n",
    "features = bow_features.todense()\n",
    "\n",
    "display_features(features, feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   and  beautiful  blue  cheese  is  love  sky  so  the\n",
      "0    0          0     1       0   0     0    1   0    0\n"
     ]
    }
   ],
   "source": [
    "new_doc = ['loving this blue sky today']\n",
    "new_doc_features = bow_vectorizer.transform(new_doc)\n",
    "new_doc_features = new_doc_features.todense()\n",
    "\n",
    "display_features(new_doc_features, feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term Frequency Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    and  beautiful  blue  cheese    is  love   sky    so   the\n",
      "0  0.00       0.00  0.40    0.00  0.49  0.00  0.49  0.00  0.60\n",
      "1  0.44       0.35  0.23    0.00  0.56  0.00  0.56  0.00  0.00\n",
      "2  0.00       0.43  0.29    0.00  0.35  0.00  0.35  0.55  0.43\n",
      "3  0.00       0.00  0.35    0.66  0.00  0.66  0.00  0.00  0.00\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF\n",
    "feature_names = bow_vectorizer.get_feature_names()\n",
    "\n",
    "tfidf_trans, tdidf_features = tfidf_transformer(bow_features)\n",
    "features = np.round(tdidf_features.todense(), 2)\n",
    "display_features(features, feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   and  beautiful  blue  cheese   is  love   sky   so  the\n",
      "0  0.0        0.0  0.63     0.0  0.0   0.0  0.77  0.0  0.0\n"
     ]
    }
   ],
   "source": [
    "# New document\n",
    "# new_doc = ['loving this blue sky today']\n",
    "nd_tfidf = tfidf_trans.transform(new_doc_features)\n",
    "nd_features = np.round(nd_tfidf.todense(), 2)\n",
    "display_features(nd_features, feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF (Custom computation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   and  beautiful  blue  cheese   is  love  sky   so  the\n",
      "0  0.0        0.0   1.0     0.0  1.0   0.0  1.0  0.0  1.0\n",
      "1  1.0        1.0   1.0     0.0  2.0   0.0  2.0  0.0  0.0\n",
      "2  0.0        1.0   1.0     0.0  1.0   0.0  1.0  1.0  1.0\n",
      "3  0.0        0.0   1.0     1.0  0.0   1.0  0.0  0.0  0.0\n"
     ]
    }
   ],
   "source": [
    "# Computing TF-IDF Matrices\n",
    "\n",
    "feature_names = bow_vectorizer.get_feature_names()\n",
    "\n",
    "# compute term frequency\n",
    "tf = bow_features.todense()\n",
    "tf = np.array(tf, dtype='float64')\n",
    "\n",
    "# show term frequencies\n",
    "display_features(tf, feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   and  beautiful  blue  cheese  is  love  sky  so  the\n",
      "0    2          3     5       2   4     2    4   2    3\n"
     ]
    }
   ],
   "source": [
    "# build the document frequency matrix\n",
    "df = np.diff(sp.csc_matrix(bow_features, copy=True).indptr)\n",
    "df = 1 + df  # to smoothen idf later\n",
    "\n",
    "# show document frequencies\n",
    "display_features([df], feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    and  beautiful  blue  cheese    is  love   sky    so   the\n",
      "0  1.92       1.51   1.0    1.92  1.22  1.92  1.22  1.92  1.51\n"
     ]
    }
   ],
   "source": [
    "# compute inverse document frequencies\n",
    "total_docs = 1 + len(CORPUS)\n",
    "idf = 1.0 + np.log(float(total_docs) / df)\n",
    "\n",
    "# show inverse document frequencies\n",
    "display_features([np.round(idf, 2)], feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.92  0.    0.    0.    0.    0.    0.    0.    0.  ]\n",
      " [ 0.    1.51  0.    0.    0.    0.    0.    0.    0.  ]\n",
      " [ 0.    0.    1.    0.    0.    0.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.    1.92  0.    0.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.    1.22  0.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.    1.92  0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    1.22  0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    1.92  0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    0.    1.51]]\n"
     ]
    }
   ],
   "source": [
    "# compute idf diagonal matrix\n",
    "total_features = bow_features.shape[1]\n",
    "idf_diag = sp.spdiags(idf, diags=0, m=total_features, n=total_features)\n",
    "idf = idf_diag.todense()\n",
    "\n",
    "# print(the idf diagonal matrix)\n",
    "print(np.round(idf, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    and  beautiful  blue  cheese    is  love   sky    so   the\n",
      "0  0.00       0.00   1.0    0.00  1.22  0.00  1.22  0.00  1.51\n",
      "1  1.92       1.51   1.0    0.00  2.45  0.00  2.45  0.00  0.00\n",
      "2  0.00       1.51   1.0    0.00  1.22  0.00  1.22  1.92  1.51\n",
      "3  0.00       0.00   1.0    1.92  0.00  1.92  0.00  0.00  0.00\n"
     ]
    }
   ],
   "source": [
    "# compute tfidf feature matrix\n",
    "tfidf = tf * idf\n",
    "\n",
    "# show tfidf feature matrix\n",
    "display_features(np.round(tfidf, 2), feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.5   4.35  3.5   2.89]\n"
     ]
    }
   ],
   "source": [
    "# compute L2 norms\n",
    "norms = norm(tfidf, axis=1)\n",
    "\n",
    "# print(norms for each document)\n",
    "print(np.round(norms, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    and  beautiful  blue  cheese    is  love   sky    so   the\n",
      "0  0.00       0.00  0.40    0.00  0.49  0.00  0.49  0.00  0.60\n",
      "1  0.44       0.35  0.23    0.00  0.56  0.00  0.56  0.00  0.00\n",
      "2  0.00       0.43  0.29    0.00  0.35  0.00  0.35  0.55  0.43\n",
      "3  0.00       0.00  0.35    0.66  0.00  0.66  0.00  0.00  0.00\n"
     ]
    }
   ],
   "source": [
    "# compute normalized tfidf\n",
    "norm_tfidf = tfidf / norms[:, None]\n",
    "\n",
    "# show final tfidf feature matrix\n",
    "display_features(np.round(norm_tfidf, 2), feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   and  beautiful  blue  cheese   is  love   sky   so  the\n",
      "0  0.0        0.0  0.63     0.0  0.0   0.0  0.77  0.0  0.0\n"
     ]
    }
   ],
   "source": [
    "# compute new doc term freqs from bow freqs\n",
    "nd_tf = new_doc_features\n",
    "nd_tf = np.array(nd_tf, dtype='float64')\n",
    "\n",
    "# compute tfidf using idf matrix from train corpus\n",
    "nd_tfidf = nd_tf * idf\n",
    "nd_norms = norm(nd_tfidf, axis=1)\n",
    "norm_nd_tfidf = nd_tfidf / nd_norms[:, None]\n",
    "\n",
    "# show new_doc tfidf feature vector\n",
    "display_features(np.round(norm_nd_tfidf, 2), feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter search query. Press \"Enter\" to stop: \n",
      "the girl who was cheating in exam\n",
      "\n",
      "************ girl cheat exam ************\n",
      "\n",
      "Document no. 400, rank: 0.224665815373118\n",
      "My name is Mahya. I swear, I did not cheat in my Data Mining exam. I totally wrote my answers by myself and never asked somebody even one question. If you doubt it you can ask my friend Flower-Bahare-Flower. \"Collusion is suspected; or, one witness for his own benefit\". Data Mining.\n",
      "\n",
      "\n",
      "\n",
      "Document no. 401, rank: 0.10329457875192848\n",
      "My name is Narges, but I prefer my name to be Flower-Bahare-Flower, please call me so. And yes, Mahya is right. I know her. She is really a truthful girl and never ever ever hided those playing cards in Taleqani. Data Mining.\n",
      "\n",
      "\n",
      "\n",
      "Document no. 244, rank: 0.045996320840406946\n",
      "pulmonary vascular plexiform lesion pathogenetic studies . an attempt was made to test the theory that in pulmonary arterial hypertension, the plexiform lesion is a jet lesion beyond points of arterial stenosis resulting from nonspecific intimal thickening . in 39 subjects with such congenital communications as are associated with pulmonary arterial hypertension, the lungs were studied histologically . in each, there was an additional element of pulmonary venous obstruction . it is likely that in the absence of pulmonary venous obstruction, adult patients with only the congenital communication would have developed plexiform lesions . in the three adult patients in the study, no plexiform lesions were identified . among the 36 infants or children, one subject showed plexiform lesions, a 53-day-old girl with mitral atresia, ventricular septal defect, and patent ductus arteriosus . while the findings in the three adult subjects support the theory regarding the genesis of plexiform lesions, the findings in the one infant with plexiform lesions appear to contradict it .\n",
      "\n",
      "\n",
      "Enter search query. Press \"Enter\" to stop: \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corpus = get_corpus('corpus400.txt')\n",
    "search(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_corpus = [\"The fox jumps over the dog\",\n",
    "              \"The fox is very clever and quick\",\n",
    "              \"The dog is slow and lazy\",\n",
    "              \"The cat is smarter than the fox and the dog\",\n",
    "              \"Python is an excellent programming language\",\n",
    "              \"Java and Ruby are other programming languages\",\n",
    "              \"Python and Java are very popular programming languages\",\n",
    "              \"Python programs are smaller than Java programs\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSI topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['fox', 'jump', 'dog'],\n",
       " ['fox', 'clever', 'quick'],\n",
       " ['dog', 'slow', 'lazy'],\n",
       " ['cat', 'smarter', 'fox', 'dog'],\n",
       " ['python', 'excellent', 'programming', 'language'],\n",
       " ['java', 'ruby', 'programming', 'language'],\n",
       " ['python', 'java', 'popular', 'programming', 'language'],\n",
       " ['python', 'program', 'small', 'java', 'program']]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_tokenized_corpus = normalize_corpus(toy_corpus, tokenize=True)\n",
    "norm_tokenized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'excellent': 9, 'clever': 3, 'small': 16, 'programming': 11, 'smarter': 8, 'cat': 7, 'fox': 0, 'popular': 15, 'java': 14, 'slow': 5, 'ruby': 13, 'dog': 1, 'quick': 4, 'lazy': 6, 'language': 12, 'program': 17, 'jump': 2, 'python': 10}\n"
     ]
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(norm_tokenized_corpus)\n",
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1)],\n",
       " [(0, 1), (3, 1), (4, 1)],\n",
       " [(1, 1), (5, 1), (6, 1)],\n",
       " [(0, 1), (1, 1), (7, 1), (8, 1)],\n",
       " [(9, 1), (10, 1), (11, 1), (12, 1)],\n",
       " [(11, 1), (12, 1), (13, 1), (14, 1)],\n",
       " [(10, 1), (11, 1), (12, 1), (14, 1), (15, 1)],\n",
       " [(10, 1), (14, 1), (16, 1), (17, 2)]]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in norm_tokenized_corpus]\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1\n",
      "0.459*\"programming\" + 0.459*\"language\" + 0.344*\"java\" + 0.344*\"python\" + 0.336*\"popular\" + 0.318*\"excellent\" + 0.318*\"ruby\" + 0.148*\"program\" + 0.074*\"small\" + -0.000*\"clever\"\n",
      "\n",
      "Topic #2\n",
      "0.459*\"fox\" + 0.459*\"dog\" + 0.444*\"jump\" + 0.322*\"cat\" + 0.322*\"smarter\" + 0.208*\"quick\" + 0.208*\"clever\" + 0.208*\"slow\" + 0.208*\"lazy\" + 0.000*\"programming\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tfidf = models.TfidfModel(corpus)\n",
    "corpus_tfidf = tfidf[corpus]\n",
    "\n",
    "total_topics = 2\n",
    "\n",
    "lsi = models.LsiModel(corpus_tfidf,\n",
    "                      id2word=dictionary,\n",
    "                      num_topics=total_topics)\n",
    "\n",
    "for index, topic in lsi.print_topics(total_topics):\n",
    "    print('Topic #' + str(index + 1))\n",
    "    print(topic)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1 with weights\n",
      "[('programming', 0.46000000000000002), ('language', 0.46000000000000002), ('java', 0.34000000000000002), ('python', 0.34000000000000002), ('popular', 0.34000000000000002), ('excellent', 0.32000000000000001), ('ruby', 0.32000000000000001), ('program', 0.14999999999999999), ('small', 0.070000000000000007)]\n",
      "\n",
      "Topic #2 with weights\n",
      "[('fox', 0.46000000000000002), ('dog', 0.46000000000000002), ('jump', 0.44), ('cat', 0.32000000000000001), ('smarter', 0.32000000000000001), ('quick', 0.20999999999999999), ('clever', 0.20999999999999999), ('slow', 0.20999999999999999), ('lazy', 0.20999999999999999)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_topics_gensim(topic_model=lsi,\n",
    "                    total_topics=total_topics,\n",
    "#                     num_terms=5,\n",
    "                    display_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSI custom built topic model (using SVD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1 with weights\n",
      "[('dog', 0.72), ('fox', 0.72), ('jump', 0.43), ('smarter', 0.34), ('cat', 0.34), ('quick', 0.23), ('clever', 0.23), ('slow', 0.23), ('lazy', 0.23)]\n",
      "\n",
      "Topic #2 with weights\n",
      "[('programming', -0.73), ('language', -0.73), ('python', -0.56), ('java', -0.56), ('popular', -0.34), ('excellent', -0.33), ('ruby', -0.33), ('program', -0.21)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "norm_corpus = normalize_corpus(toy_corpus)\n",
    "\n",
    "vectorizer, tfidf_matrix = build_feature_matrix(norm_corpus,\n",
    "                                                feature_type='tfidf')\n",
    "td_matrix = tfidf_matrix.transpose()\n",
    "\n",
    "td_matrix = td_matrix.multiply(td_matrix > 0)\n",
    "\n",
    "total_topics = 2\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "u, s, vt = low_rank_svd(td_matrix, singular_count=total_topics)\n",
    "weights = u.transpose() * s[:, None]\n",
    "\n",
    "topics = get_topics_terms_weights(weights, feature_names)\n",
    "\n",
    "print_topics_udf(topics=topics,\n",
    "                 total_topics=total_topics,\n",
    "                 weight_threshold=0.15,\n",
    "                 display_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1 with weights\n",
      "[('programming', 0.55), ('language', 0.55), ('python', 0.4), ('java', 0.4), ('popular', 0.24), ('ruby', 0.23), ('excellent', 0.23), ('program', 0.09), ('small', 0.03)]\n",
      "\n",
      "Topic #2 with weights\n",
      "[('dog', 0.57), ('fox', 0.57), ('jump', 0.35), ('smarter', 0.26), ('cat', 0.26), ('quick', 0.13), ('slow', 0.13), ('clever', 0.13), ('lazy', 0.13)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "norm_corpus = normalize_corpus(toy_corpus)\n",
    "vectorizer, tfidf_matrix = build_feature_matrix(norm_corpus,\n",
    "                                                feature_type='tfidf')\n",
    "total_topics = 2\n",
    "nmf = NMF(n_components=total_topics,\n",
    "          random_state=42, alpha=.1, l1_ratio=.5)\n",
    "nmf.fit(tfidf_matrix)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "weights = nmf.components_\n",
    "\n",
    "topics = get_topics_terms_weights(weights, feature_names)\n",
    "print_topics_udf(topics=topics,\n",
    "                 total_topics=total_topics,\n",
    "                 num_terms=None,\n",
    "                 display_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
